<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Multimodal Autoregressive Unsupervised Learning | Blog of James Brown</title>
  <meta name="author" content="James Brown">
  
  <meta name="description" content="The article delves into various topics including multimodal unsupervised learning, Kaggle login security, token encoding with GPT2 and LLaMA, ViT adaptation, Genie architecture, prompt-based image/video editing, and cautions against NTFS usage in Linux. It also offers detailed instructions on PyTorch GPU installation, environment variable settings, manual token encoding, custom embedding, and exploring multimodal learning applications.">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Multimodal Autoregressive Unsupervised Learning"/>
  <meta property="og:site_name" content="Blog of James Brown"/>

  
    <meta property="og:image" content=""/>
  

  
    <link rel="alternative" href="/blog/atom.xml" title="Blog of James Brown" type="application/atom+xml">
  
  
    <link href="/blog/favicon.png" rel="icon">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/blog/css/themes/cerulean.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/highlight-default.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/google-fonts.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/comment.css" media="screen" type="text/css">
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.7/es5-sham.min.js"></script>
  <![endif]-->

  <script src="/blog/js/jquery-2.0.3.min.js"></script>
  
  
  <!-- analytics -->
  



<meta name="generator" content="Hexo 7.0.0"></head>

<body>
  <nav id="main-nav" class="navbar navbar-inverse navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
	<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="navbar-brand" href="/blog/">Blog of James Brown</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/blog/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/blog/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/blog/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/blog/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
    <div class="content">
      


	
		<div class="page-header page-header-inverse ">		
			<h1 class="title title-inverse "> Multimodal Autoregressive Unsupervised Learning</h1>
		</div>		
	






<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  
		 <div class="alert alert-success description">
			<i class="fa fa-info-circle"></i> <p>The article delves into various topics including multimodal unsupervised learning, Kaggle login security, token encoding with GPT2 and LLaMA, ViT adaptation, Genie architecture, prompt-based image&#x2F;video editing, and cautions against NTFS usage in Linux. It also offers detailed instructions on PyTorch GPU installation, environment variable settings, manual token encoding, custom embedding, and exploring multimodal learning applications.</p>
			
		 </div> <!-- alert -->
	  		

	  <p>It is so good that I can login to Kaggle on smartphone.</p>
<hr>
<p>Instruction following image editing via prompt engineering like <a target="_blank" rel="noopener" href="https://minidalle3.github.io/">Mini DALLE3</a> or multimodal model like <a target="_blank" rel="noopener" href="https://github.com/THUDM/CogCoM">CogCoM</a></p>
<hr>
<p>Recently search engine &amp; browser augmented generation has become popular. A great tool for creating video scripts.</p>
<hr>
<p>Google has released a new architecure called Genie, which can generate latent action space only using video unsupervised training.</p>
<hr>
<p>Do not ever use NTFS in Linux. If you unfortunatedly face disk inaccessible problem when accessing NTFS disks, in the first case run chkdsk &#x2F;f onthen reboot into Windows twice. The usage of the &#x2F;f parameter iimportant!</p>
<p>After fixing, copy all files to another place, format the disk into ext4 or xfs, then recover the files.</p>
<hr>
<p>GPT2 models from huggingface accept <code>inputs_embeds</code> as parameter of instance call and method “generate”.</p>
<p>Typically to adapt ViT into LLM you need <code>LayerNorm</code> and a linear projection layer.</p>
<p>You cannot put custom embedding into text generaton pipeline.</p>
<hr>
<p>To encode token manually in GPT2:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs_embeds = model.transformer.wte(input_ids)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>In LLaMA:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">embed_tokens</span>(<span class="params">self，token_ids</span>):</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">hasattr</span>(self.llama_model.base_model, <span class="string">&quot;model&quot;</span>): <span class="comment"># with lora</span></span><br><span class="line">embeds = self.llama_model.base_model.model.model.embed_tokens(token_ids)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">self.llama_model.base_model.embed_tokens(token_ids)</span><br><span class="line"><span class="keyword">return</span> embeds</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<p>Install pytorch gpu with conda:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<p>Set <code>HF_HUB_OFFLINE=1</code> while loading local models, to prevent accessing network.</p>
<hr>
<p>Set <code>Environment=&quot;OLLAMA_MODELS=&lt;model_storage_path&gt;&quot;</code> in ollama systemd service file. Remember to change username and usergroup too, and set appropriate permission to model storage path.</p>
<p>In Windows, set it in system environment variables, or create a directory symlink by:</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mklink /D C:\Users\&lt;User&gt;\.ollama\models E:\AI\Ollama\Models</span><br><span class="line"></span><br></pre></td></tr></table></figure>
	  
	</div>

	<!-- recommended posts -->
	

	<div>
  	<center>
	<div class="pagination">
<ul class="pagination">
	 
				
		<li class="prev"><a href="/blog/2024/02/29/df1142cf-7353-4e88-bae0-c318b780c790/" class="alignleft prev"><i class="fa fa-arrow-circle-o-left"></i>Prev</a></li>
  		

        <li><a href="/blog/archives"><i class="fa fa-archive"></i>Archive</a></li>

		
          <li class="next"><a href="/blog/2024/02/26/e35b7a12-943e-41d2-b1d3-e40725945e45/" class="alignright next">Next<i class="fa fa-arrow-circle-o-right"></i></a></li>
        
	
</ul>
</div>

    </center>
	</div>

    <!-- share -->
    
        
    <div class="bdsharebuttonbox">
        <a href="#" class="bds_more" data-cmd="more"></a>
        <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
        <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
        <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
        <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
        <a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a>
        <a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a>
        <a href="#" class="bds_youdao" data-cmd="youdao" title="分享到有道云笔记"></a>
        <a href="#" class="bds_copy" data-cmd="copy" title="分享到复制网址"></a>
    </div>
    <script>
        window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{}};
        with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
    </script>


        

    
	
	<!-- comment -->
	
<section id="comment">
  <h2 class="title">Comments</h2>
  
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2024-02-28 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/blog/categories/Multimodal-Unsupervised-Learning/">Multimodal Unsupervised Learning<span>1</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/blog/tags/multimodal-unsupervised-learning/">multimodal unsupervised learning<span>1</span></a></li> <li><a href="/blog/tags/Kaggle-login-security/">Kaggle login security<span>1</span></a></li> <li><a href="/blog/tags/GPT2-and-LLaMA-token-encoding/">GPT2 and LLaMA token encoding<span>1</span></a></li> <li><a href="/blog/tags/ViT-adaptation/">ViT adaptation<span>1</span></a></li> <li><a href="/blog/tags/Genie-architecture/">Genie architecture<span>1</span></a></li> <li><a href="/blog/tags/image-video-editing-with-prompts/">image/video editing with prompts<span>1</span></a></li> <li><a href="/blog/tags/warnings-against-NTFS-in-Linux/">warnings against NTFS in Linux<span>1</span></a></li> <li><a href="/blog/tags/PyTorch-GPU-installation/">PyTorch GPU installation<span>1</span></a></li> <li><a href="/blog/tags/environment-variable-settings/">environment variable settings<span>1</span></a></li> <li><a href="/blog/tags/manual-token-encoding/">manual token encoding<span>1</span></a></li> <li><a href="/blog/tags/embedding-customization/">embedding customization<span>1</span></a></li> <li><a href="/blog/tags/multimodal-learning-applications/">multimodal learning applications<span>1</span></a></li>

    </ul>
	</div>
	

	<!-- toc -->
	<div class="meta-widget">
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->




    </div>
  </div>
  <div class="container-narrow">
    <footer> <p>
  &copy; 2024 James Brown
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p> </footer>
  </div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/blog/js/jquery.imagesloaded.min.js"></script>
<script src="/blog/js/gallery.js"></script>
<script src="/blog/js/bootstrap.min.js"></script>
<script src="/blog/js/main.js"></script>
<script src="/blog/js/search.js"></script> 


<link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/blog/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/blog/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>


<!-- syntax highlighting -->


</body>
</html>