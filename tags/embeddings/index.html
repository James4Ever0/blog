<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>embeddings | Blog of James Brown</title>
  <meta name="author" content="James Brown">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Blog of James Brown"/>

  
    <meta property="og:image" content=""/>
  

  
    <link rel="alternative" href="/blog/atom.xml" title="Blog of James Brown" type="application/atom+xml">
  
  
    <link href="/blog/favicon.png" rel="icon">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/blog/css/themes/cerulean.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/highlight-default.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/google-fonts.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/comment.css" media="screen" type="text/css">
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.7/es5-sham.min.js"></script>
  <![endif]-->

  <script src="/blog/js/jquery-2.0.3.min.js"></script>
  
  
  <!-- analytics -->
  



<meta name="generator" content="Hexo 7.0.0"></head>

<body>
  <nav id="main-nav" class="navbar navbar-inverse navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
	<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="navbar-brand" href="/blog/">Blog of James Brown</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/blog/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/blog/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/blog/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/blog/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
    <div class="content">
      

<!-- title -->
<div class="page-header page-header-inverse ">
  <h1 class="archive-title-tag title title-inverse ">embeddings</h1>
</div>

<div class="row page">
  <!-- cols -->
  
  <div class="col-md-9">
	

	  <div id="top_search"></div>

      
         <!-- display as entry -->
	     <div class="mypage">
	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2024-03-01 </div>
			<div class="article-title"><a href="/blog/2024/03/01/e2a428f9-f040-423f-b3b5-fbebd808e526/" title="This article explores the utilization of IPYthon for code testing and image processing in language models, with an emphasis on techniques like `patchify` and Torch&#39;s `unfold`. It delves into converting patches into embeddings using LayerNorm and Dense layers, specifically focusing on the class token as a full image summary. Additionally, it introduces speaker detection models such as pyannote and diart for audio processing, demonstrating their implementation with Whisper&#39;s architecture and AST transformers in Python.">Image And Audio Feature Extraction For Language Models</a></div>
		</h3>
	


		     
<div class="entry">

  <div class="row">
	
	
		<p>Use <code>ipython</code> instead of <code>python</code> to test these code, get better parameter hints and dynamic hints, just like what you saw in <code>brownie</code>.</p>
<p>Use <code>dataset.transform</code> instead of <code>dataset.map</code> to save loading time.</p>
<h2 id="Image-processing"><a href="#Image-processing" class="headerlink" title="Image processing"></a>Image processing</h2><p>Many language models resize, reshape &amp; pad the input image into 224x224 square and put into ViT directly.</p>
<p>To simplify the pipeline, we would recommend you to sample the image into fixed size square patches, like 2x2, 4x4 etc.</p>
<p>Or you can skip the ViT embedding part, just use <a target="_blank" rel="noopener" href="https://hf-mirror.com/adept/fuyu-8b">Fuyu-8b</a> or take its architecture <code>FuyuForCausalLM</code> and processor <code>FuyuProcessor</code> because it supports arbitrary sized images.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> FuyuProcessor, FuyuForCausalLM</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">model_name = <span class="string">&quot;adept/fuyu-8b&quot;</span></span><br><span class="line">processor = FuyuProcessor.from_pretrained(model_name)</span><br><span class="line">model = FuyuForCausalLM.from_pretrained(model_name)</span><br><span class="line">url = <span class="string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(requests.get(url, stream=<span class="literal">True</span>).raw)</span><br><span class="line">prompt = <span class="string">&quot;Generate a coco-style caption.\n&quot;</span></span><br><span class="line">inputs = processor(text=text_prompt, images=image, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">outputs = model(**inputs)</span><br><span class="line">generated_ids = model.generate(**model_inputs, max_new_tokens=<span class="number">7</span>)</span><br><span class="line">generation_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(generation_text)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Split-image-into-patches"><a href="#Split-image-into-patches" class="headerlink" title="Split image into patches"></a>Split image into patches</h3><p>Usually images are large so we need to split.</p>
<p>You have three ways to split an image.</p>
<h4 id="Patchify"><a href="#Patchify" class="headerlink" title="Patchify"></a>Patchify</h4><p>The splited indexs are put in front instead of appended back.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> patchify <span class="keyword">import</span> patchify</span><br><span class="line">image = np.random.rand(<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>)</span><br><span class="line">patches = patchify(image, (<span class="number">128</span>,<span class="number">128</span>,<span class="number">3</span>), step=<span class="number">128</span>)</span><br><span class="line"><span class="built_in">print</span>(patches.shape) <span class="comment"># (4, 4, 1, 128, 128, 3)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="Torch-unfold"><a href="#Torch-unfold" class="headerlink" title="Torch unfold"></a>Torch <code>unfold</code></h4><p>It works by expanding target dimension and appending a new dimension corresponding to it.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">image = torch.rand(<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>)</span><br><span class="line">patches = image.unfold(<span class="number">0</span>, <span class="number">128</span>, <span class="number">128</span>).unfold(<span class="number">1</span>, <span class="number">128</span>, <span class="number">128</span>).unfold(<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(patches.shape) <span class="comment"># torch.Size([4, 4, 1, 128, 128, 3])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="EMPatches"><a href="#EMPatches" class="headerlink" title="EMPatches"></a>EMPatches</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> empatches <span class="keyword">import</span> EMPatches</span><br><span class="line">image = np.random.rand(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>)</span><br><span class="line">emp = EMPatches()</span><br><span class="line">patches, indices = emp.extract_patches(image, patchsize = <span class="number">128</span>, overlap = <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(patches) <span class="comment"># a list of numpy arrays, total 16 items</span></span><br><span class="line"><span class="built_in">print</span>(indices) <span class="comment"># [(x_start, x_end, y_start, y_end), ...], total 16 items</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Convert-fixed-size-patches-into-embeddings"><a href="#Convert-fixed-size-patches-into-embeddings" class="headerlink" title="Convert fixed-size patches into embeddings"></a>Convert fixed-size patches into embeddings</h3><p>The embeddings from ViT cannot be used directly by LLM. Instead, use <code>LayerNorm</code> and <code>Dense</code> as simple adaptors.</p>
<p>The first token is the class token, randomly initialized and processed along with the transformer, output as the summary of the full image, can be extracted for image embedding.</p>
<p>Proof:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">224x224 is the shape of input image</span><br><span class="line">16x16 is the patch size</span><br><span class="line">224/16 = 14</span><br><span class="line">14*14 + 1 = 197</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"><span class="comment"># not torch.randn (sample from normal distribution)</span></span><br><span class="line">image = torch.rand(<span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>) <span class="comment"># chw</span></span><br><span class="line">model_name = <span class="string">&quot;google/vit-base-patch16-224-in21k&quot;</span></span><br><span class="line">processor = transformers.AutoImageProcessor(model_name) <span class="comment"># for processing image</span></span><br><span class="line">image = processor(image, do_rescale=<span class="literal">False</span>) <span class="comment"># use this parameter when passing values ranging from 0 to 1</span></span><br><span class="line"><span class="comment">#image = processor(pil_image) # can also handle pil image</span></span><br><span class="line">model = transformers.ViTModel(model_name)</span><br><span class="line">outputs = model(pixel_values = image)</span><br><span class="line">embeddings = outputs.last_hidden_state[:,<span class="number">0</span>,:] <span class="comment"># torch.Size([1, 768])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Audio-processing"><a href="#Audio-processing" class="headerlink" title="Audio processing"></a>Audio processing</h2><p>An useful and related field to speaker diarization in video processing is visual entity recognization, which can help you identify anime or movie characters across different frames.</p>
<p>When unsure, the agent shall consult online search engines, subtitles and existing recognized entities for classification. If a dataset is successfully created, one can train a YOLO model to speed up the process, used along with popular person&#x2F;anime head detection models.</p>
<p>In most videos speakers and visuals are aligned. You can first identify speakers then get character identification. Remember you need to use special pipeline for long-time diarization, sharing speaker features for cross-audio diarization.</p>
<hr>
<p>For multilanguage context, you would like to use speaker detection models like <a target="_blank" rel="noopener" href="https://github.com/pyannote/pyannote-audio">pyannote</a>. <a target="_blank" rel="noopener" href="https://github.com/juanmc2005/diart">Diart</a> is a speech processing library based on that and can be used in real time, with speaker diarization, voice activity detection training pipelines.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/ufal/whisper_streaming">Whisper-streaming</a> uses LocalAgreement algoritm to segment chunks of audio and merge common patterns.</p>
<hr>
<p>Whisper architecture is comprised of an audio encoder and transcription decoder. The output of the encoder is feed into every cross attention layer of the decoder. For feature extraction, you only need to use the encoder.</p>
<hr>
<p>You pass single channel audio amplitude array to audio feature extractors with predetermined audio sample rate. If the sample rate mismatch, you need to resample the audio.</p>
<hr>
<p>Different audio transformers choose different context window sizes. Like LLMs, they can be streamed. However during training they must use a fixed context size.</p>
<p>For <a target="_blank" rel="noopener" href="https://github.com/OpenAI/whisper">Whisper</a>, the context size is 30 seconds. Confugurable at: <code>transformers.WhisperFeatureExtractor(chunk_length=30, ...)</code></p>
<p>For AST, it is 10.24 seconds. You can find more info about input and output sizes <a target="_blank" rel="noopener" href="https://github.com/YuanGongND/ast">here</a>. Configurable at: <code>transformers.ASTFeatureExtractor(max_length=1024, ...)</code></p>
<p>These numbers can be found over respective processor parameters.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoProcessor, ASTModel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> dataset <span class="keyword">import</span> load_dataset</span><br><span class="line">dataset_name = <span class="string">&quot;hf_internal_testing/librispeech_asr_demo&quot;</span></span><br><span class="line">model_name = <span class="string">&quot;MIT/ast-finetuned-audioset-10-10-0.4593&quot;</span></span><br><span class="line">dataset = load_dataset(dataset_name, <span class="string">&#x27;clean&#x27;</span>, split=<span class="string">&quot;validation&quot;</span>)</span><br><span class="line">sampling_rate = dataset.features[<span class="string">&quot;audio&quot;</span>].sampling_rate</span><br><span class="line">processor = AutoProcessor.from_pretrained(model_name)</span><br><span class="line">model = ASTModel.from_pretrained(model_name)</span><br><span class="line">audio_array = dataset[<span class="number">0</span>].audio.array</span><br><span class="line">inputs = processor(audio_array, sampling_rate=sampling_rate, return_tensors=<span class="string">&#x27;pt&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">outputs = model(**inputs)</span><br><span class="line">pooler_output = outputs[<span class="string">&quot;pooler_output&quot;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

	
	</div>
  <a type="button" href="/blog/2024/03/01/e2a428f9-f040-423f-b3b5-fbebd808e526/#more" class="btn btn-default more">Read More</a>
</div>

	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2023-05-04 </div>
			<div class="article-title"><a href="/blog/2023/05/04/d361bf81-d50c-4031-af5e-cf0d860f5bff/" title="This article provides detailed information on a wide range of topics, including creating tokenizers and embeddings using ChatGPT-derived projects, remotely controlling OBS, handling complex neural networks, generating multimodal data output, utilizing LoRA for AI model performance improvement, annotating datasets, developing task-specific embeddings in classification models, working with various libraries for managing neural networks, considering VNC server factors, using `webdav-cli`, recording video on Ubuntu ARM VM, adjusting display settings in UTM VMs, and resizing UTM VM disks via virtio disk resizing and Gparted.">Agi That Controls Computer</a></div>
		</h3>
	


		     
<div class="entry">

  <div class="row">
	
	
		<p>make specialized (in RPA) tokenizer and embedding for this new model. add new words to the tokenizer.</p>
<hr>
<p>you can just boot ubuntu&#x2F;kali&#x2F;parrot iso without installing.</p>
<p>but that would make us embarrasing. we need to check for the option.</p>
<hr>
<p>use ChatGPT-derived projects for localized propaganda on CyberGod and The Frozen Forest.</p>
<h2 id="obs-remote-control"><a href="#obs-remote-control" class="headerlink" title="obs remote control"></a>obs remote control</h2><p>using <a target="_blank" rel="noopener" href="https://github.com/obsproject/obs-websocket">obs-websocket</a> you can use python to do real scripting. but first spin up obs first (with websocket related commandline arguments)</p>
<p>launch obs in minimized way <code>obs --minimize-to-tray</code> or just using xvfb.</p>
<p>you can also write and load scripts for obs, run on custom intervals and conditions.</p>
<h2 id="audio-recording"><a href="#audio-recording" class="headerlink" title="audio recording"></a>audio recording</h2><p>your OS may go slient if you want to record audio from “speakers”</p>
<hr>
<p>using pyaudio, on macos, you need blackhole for sending all audio to oblivion, thus able to be recorded.</p>
<p>on Linux, you need audio loopback device.</p>
<p>run: <code>sudo modprobe snd-aloop</code></p>
<p>you use <code>hw:1:0</code> or “Analog Device Output” for slient output&#x2F;speaker, and use <code>hw:1:1</code> or “Analog Device Input” for recording.</p>
<h2 id="benchmarks"><a href="#benchmarks" class="headerlink" title="benchmarks"></a>benchmarks</h2><p>it is always a mystery for us to develop the right ML model. however, we can setup guidelines of good performance over specific task.</p>
<p>automate the benchmark, setup metrics. there could be more room for trials and imagination.</p>
<h2 id="encoding"><a href="#encoding" class="headerlink" title="encoding"></a>encoding</h2><p>use hfft&#x2F;rfft to transform multipart inputs (special bits, different part of mouse coords (x, y, dx, dy))</p>
<p>if you want to use complex number as RNN input, you may need to swap ViT for ComplexConv2D, but maybe you just need a few.</p>
<hr>
<p>libraries that handle complex neural networks:</p>
<p><a target="_blank" rel="noopener" href="https://github.com/wavefrontshaping/complexPyTorch">complexPyTorch</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/soumickmj/pytorch-complex">pytorch-complex</a></p>
<h2 id="multimodal"><a href="#multimodal" class="headerlink" title="multimodal"></a>multimodal</h2><p>do our model have to output multimodal data?</p>
<p>if you combine some “special” bits along with token embeding by ihfft, you may have to retrain the entire damn network. also in order to make way for special bits, you may have to introduce extra linear layer.</p>
<hr>
<p>some may prefer “LoRA”? by only introducing few tunable params and changing the overall output?</p>
<hr>
<p>we may not annotate anything in our dataset. in contrast, we will set goals and make multiple interfaces for our model to explore.</p>
<hr>
<p>you can add special task specific embedding before passing to main model, then minus that task specific embedding after passing to classification model.</p>
<h2 id="file-sharing-and-communication"><a href="#file-sharing-and-communication" class="headerlink" title="file sharing and communication"></a>file sharing and communication</h2><p>make sure you don’t share important files as read&#x2F;write on VM.</p>
<hr>
<p>you may host some “execution server” on UTM VMs. you may expose your very large hard disk using WebDAV server. i think x11vnc and other vnc server may suffice for linux, but we always want to listen to the real operational data, including human operation&#x2F;intervention, not just those in VNC protocols.</p>
<hr>
<p>WebDAV servers:</p>
<p><a target="_blank" rel="noopener" href="https://github.com/mar10/wsgidav">wsgidav</a> (python)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wsgidav --host=192.168.64.1 --port=8081 --root=<span class="string">&quot;/Volumes/Toshiba XG3/works/agi_computer_control&quot;</span>  --auth=anonymous</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://github.com/svtslv/webdav-cli">webdav-cli</a> （nodejs)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">webdav-cli --host=192.168.64.1 --port=8081 --username=root --password=root --path=<span class="string">&quot;/Volumes/Toshiba XG3/works/agi_computer_control&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="video-recording"><a href="#video-recording" class="headerlink" title="video recording"></a>video recording</h2><p>for Ubuntu ARM VM, <code>mss</code> failed on wayland but <code>pyautogui</code> works in both cases. write one python script to pipe raw images to ffmpeg for better compression ratio by shell. the final video is not “time-accurate”. it is frame by frame, matched with timestamps.</p>
<hr>
<p>forcing ubuntu to use xorg by: <code>sudo vim /etc/gdm3/custom.conf</code></p>
<h2 id="resize-UTM-VM-disks"><a href="#resize-UTM-VM-disks" class="headerlink" title="resize UTM VM disks"></a>resize UTM VM disks</h2><p>you need to first resize the virtio disk in utm setting, then resize partition by using gparted, then <a target="_blank" rel="noopener" href="https://www.albertyw.com/note/resizing-ubuntu-utm#:~:text=For%20an%20Ubuntu%20guest%20OS%20running%20a%20default,be%20corrected%20by%20w%20%28rite%29%20warning%20More%20items">update the device mapper</a></p>

	
	</div>
  <a type="button" href="/blog/2023/05/04/d361bf81-d50c-4031-af5e-cf0d860f5bff/#more" class="btn btn-default more">Read More</a>
</div>

	       
	     </div>
	     <div>
	       <center>
	         <div class="pagination">
<ul class="pagination">
	 
</ul>
</div>

	       </center>
	     </div>	
      

</div> <!-- col-md-9/col-md-12 -->


<div class="col-md-3">
	<div id="sidebar">
	
			
  <div id="site_search">
   <div class="form-group">
    <input type="text" id="local-search-input" name="q" results="0" placeholder="Search" class="st-search-input st-default-search-input form-control"/>
   </div>  
  <div id="local-search-result"></div>
  </div>


		
			
<div class="widget">
	<h4>Links</h4>
	<ul class="blogroll list-unstyled">
	
		<li><i class="fa fa-github"></i><a href="https://github.com/james4ever0/agi_computer_control/" title="Autonomous computer agent" target="_blank"]);">Project Cybergod</a></li>
	
		<li><i class="fa fa-github"></i><a href="https://github.com/james4ever0/pyjom/" title="Media content automation" target="_blank"]);">Project Pyjom</a></li>
	
		<li><i class="fa fa-github"></i><a href="https://github.com/james4ever0/prometheous/" title="Automated documentation, AI+IR(RAG)" target="_blank"]);">Project Prometheus</a></li>
	
		<li><i class="fa fa-github"></i><a href="https://github.com/james4ever0/pyjom/" title="Media Content Automation" target="_blank"]);">Project Pyjom</a></li>
	
		<li><i class="fa fa-github"></i><a href="https://github.com/james4ever0/my_blog_source/" title="Source code of my blog"" target="_blank"]);">Blog Source Code</a></li>
	
		<li><i class="fa fa-github"></i><a href="http://www.github.com/james4ever0" title="My Github account" target="_blank"]);">My Github</a></li>
	
		<li><i class="fa fa-book"></i><a href="https://samoyedsun.github.io/" title="Samoyedsun's Blog" target="_blank"]);">Samoyedsun&#39;s Blog</a></li>
	
		<li><i class="fa fa-book"></i><a href="http://atlant1c.cn/" title="Atlant1c's Blog" target="_blank"]);">Atlant1c&#39;s Blog</a></li>
	
		<li><i class="fa fa-book"></i><a href="https://www.gregoryuan.com/" title="Gregoryuan's Blog" target="_blank"]);">Gregoryuan&#39;s Blog</a></li>
	
		<li><i class="fa fa-book"></i><a href="https://yubingtao.netlify.app/" title="Yubingtao's Blog" target="_blank"]);">Yubingtao&#39;s Blog</a></li>
	
	</ul>
</div>


		
	</div> <!-- sidebar -->
</div> <!-- col-md-3 -->




    </div>
  </div>
  <div class="container-narrow">
    <footer> <p>
  &copy; 2024 James Brown
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p> </footer>
  </div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/blog/js/jquery.imagesloaded.min.js"></script>
<script src="/blog/js/gallery.js"></script>
<script src="/blog/js/bootstrap.min.js"></script>
<script src="/blog/js/main.js"></script>
<script src="/blog/js/search.js"></script> 


<link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/blog/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/blog/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>


<!-- syntax highlighting -->


</body>
</html>