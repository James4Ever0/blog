<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>pytorch | Blog of James Brown</title>
  <meta name="author" content="James Brown">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Blog of James Brown"/>

  
    <meta property="og:image" content=""/>
  

  
    <link rel="alternative" href="/blog/atom.xml" title="Blog of James Brown" type="application/atom+xml">
  
  
    <link href="/blog/favicon.png" rel="icon">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/blog/css/themes/cerulean.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/highlight-default.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/google-fonts.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/comment.css" media="screen" type="text/css">
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.7/es5-sham.min.js"></script>
  <![endif]-->

  <script src="/blog/js/jquery-2.0.3.min.js"></script>
  
  
  <!-- analytics -->
  



<meta name="generator" content="Hexo 7.0.0"></head>

<body>
  <nav id="main-nav" class="navbar navbar-inverse navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
	<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="navbar-brand" href="/blog/">Blog of James Brown</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/blog/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/blog/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/blog/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/blog/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
    <div class="content">
      

<!-- title -->
<div class="page-header page-header-inverse ">
  <h1 class="archive-title-tag title title-inverse ">pytorch</h1>
</div>

<div class="row page">
  <!-- cols -->
  
  <div class="col-md-9">
	

	  <div id="top_search"></div>

      
         <!-- display as entry -->
	     <div class="mypage">
	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2022-08-26 </div>
			<div class="article-title"><a href="/blog/2022/08/26/6d619fa1-df90-49cb-8f3f-7492efbd5d6c/" title="In this article, you will learn how to install and use im2latex with TensorFlow and PyTorch. Additionally, the article covers important safety precautions and potential hardware damage considerations when running &#39;im2latex-tensorflow&#39;. By following these instructions, you can ensure a safe and successful implementation of im2latex in your TensorFlow or PyTorch projects.">On Building The Lua Torch Library</a></div>
		</h3>
	


		     
<div class="entry">

  <div class="row">
	
	
		<h2 id="im2latex-tensorflow-sucks-looking-for-alternatives"><a href="#im2latex-tensorflow-sucks-looking-for-alternatives" class="headerlink" title="im2latex-tensorflow sucks, looking for alternatives"></a>im2latex-tensorflow sucks, looking for alternatives</h2><p><strong>training on gpu is intensive and will occasionally burn hardware if not careful, doing this on kaggle or modify the software to stop training when gpu goes hot, but we are using trainer here</strong></p>
<p><a target="_blank" rel="noopener" href="http://nlp.seas.harvard.edu/code/">harvard nlp showcase</a></p>
<p>for those doesn’t provide pretrained models:</p>
<p><a target="_blank" rel="noopener" href="https://github.com/guillaumegenthial/im2latex">im2latex</a> in tensorflow, with makefile support, run on tensorflow v1 and python3</p>
<p><a target="_blank" rel="noopener" href="https://github.com/luopeixiang/im2latex">im2latex</a> in pytorch, more recent. the dataset has relocated to <a target="_blank" rel="noopener" href="https://zenodo.org/record/56198#.YwtB9PcRU5t">here</a> according to <a target="_blank" rel="noopener" href="https://im2markup.yuntiandeng.com/">official website</a></p>
<h2 id="install-or-run-python2-7-to-run-im2latex-tensorflow"><a href="#install-or-run-python2-7-to-run-im2latex-tensorflow" class="headerlink" title="install or run python2.7 to run im2latex-tensorflow"></a>install or run python2.7 to run <a target="_blank" rel="noopener" href="https://github.com/ArminKaramzade/im2latex">im2latex-tensorflow</a></h2><p>you may need to adapt our modified code to load the weights and test the result against our image.</p>
<p>it is reported the performance is poor. maybe it does not worth trying.</p>
<p>download tensorflow 0.12.0 for macos <a target="_blank" rel="noopener" href="https://pypi.org/project/tensorflow/0.12.0/#files">here</a></p>
<p>visit <a target="_blank" rel="noopener" href="https://docs.conda.io/en/latest/miniconda.html">here</a> to get all miniconda installers</p>
<p>to install on macos, download the installer <a target="_blank" rel="noopener" href="https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.pkg">here</a></p>
<p>some tutorial <a target="_blank" rel="noopener" href="https://blog.balasundar.com/install-older-versions-of-python-using-miniconda-on-mac-m1">here</a> about libmagic as bonus tips</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CONDA_SUBDIR=osx-64 conda create -n py27 python=2.7  <span class="comment"># include other packages here</span></span><br><span class="line"><span class="comment"># ensure that future package installs in this env stick to &#x27;osx-64&#x27;</span></span><br><span class="line">conda activate py27</span><br><span class="line">conda config --<span class="built_in">env</span> --<span class="built_in">set</span> subdir osx-64</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>after that, do this to get pip on python2.7 (rosetta2)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl https://bootstrap.pypa.io/pip/2.7/get-pip.py -o get-pip.py</span><br><span class="line">python get-pip.py</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>install tensorflow version below 1, and doing this can be far more easier on linux. maybe we should do this in conda virtual enviorment to prevent conflicts.</p>
<h2 id="we-are-doing-this-for-the-original-lua-implementation-of-im2markup"><a href="#we-are-doing-this-for-the-original-lua-implementation-of-im2markup" class="headerlink" title="we are doing this for the original lua implementation of im2markup"></a>we are doing this for the original lua implementation of <a target="_blank" rel="noopener" href="https://github.com/harvardnlp/im2markup">im2markup</a></h2><p><strong>it works!</strong></p>
<p>download libcudnn5 for torch</p>
<p>remember to activate torch enviorment by exporting the path to some shell script</p>
<p>difference between cudamalloc and cudamallocasync, and that’s some copying and pasting about some generalized template of memory manager function</p>
<p>qt4 uses CRLF so convert all text files using dos2unix</p>
<p>need to hack qt4 files to build qt4</p>
<p>hack luarocks to allow install from local spec file and download repo from github via https</p>
<p>hack some lua torch file to be compatible with cuda11</p>
<p>about c++ tweaks:</p>
<p>add ‘+’ to force type inference</p>
<p>force type conversion by using brackets</p>
<p>some macro to disable some blocks of code</p>

	
	</div>
  <a type="button" href="/blog/2022/08/26/6d619fa1-df90-49cb-8f3f-7492efbd5d6c/#more" class="btn btn-default more">Read More</a>
</div>

	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2022-04-28 </div>
			<div class="article-title"><a href="/blog/2022/04/28/48acae1c-c31d-4b17-ae00-97f8f1000a7f/" title="This article delves into designing efficient cooling systems for GPUs, implementing fire safety measures in high-powered computers, and methods to train large language models with low power consumption using Pytorch Lightning. Additionally, it explores modifying NVIDIA Tesla M40 GPUs for deep learning applications, discussing compatibility, virtualization, and purchasing considerations.">Gpt-2 Ram Requirements</a></div>
		</h3>
	


		     
<div class="entry">

  <div class="row">
	
	
		<p>do not plug the fanless gpu into the slot before testing the blower. if the wind comes out from the other side, you are good.</p>
<hr>
<p>design a external cooling system for passive cooling GPUs like P40, M40.</p>
<p>installed by screws, powered by usb cabels, sealed by heat-resistant silicon gels</p>
<p>you should adapt for different fan sizes, so that different customers can select the best ones.</p>
<p>the fan can also be screw-free by attaching it to a 3 DOF robot hand.</p>
<p>you can use plastic and glue instead of screws.</p>
<p>elaborate the structure of the fan mount by using cardboards.</p>
<p>download existing fan mount designs as reference and design your own.</p>
<p>design something like:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">_________________      fan mount bracket (slightly shorter than the gpu bracket)</span><br><span class="line">| | |                 |    /</span><br><span class="line">| |_|                 |__   original pci-e bracket</span><br><span class="line">|________________________ /</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>visit <a target="_blank" rel="noopener" href="https://yeggi.com/q/pcie+bracket">here</a> for downloading free pci-e bracket designs.</p>
<hr>
<p>服务器重量在25公斤以上 运输和搬运均需注意</p>
<p>服务器制造的热风需要用空调降温 或者需要有专门的通风管道</p>
<p>服务器的功耗是非常大的 服务器标配暴力风扇 虽然降温稳定性能很好 可以随时更换 但是噪音非常大 如果插上了不支持的显卡那么风扇会高速旋转 可能需要转接卡或者适配装置 待机功耗300w起步 而一般的台式机待机100w 如果加装了显卡那么功耗还会继续上升 同时满载的m1 ultra的mac studio功耗在140w-200w左右 相对比较节能</p>
<p>服务器需要标配灭火装置和烟雾报警装置</p>
<p>服务器需要放置在隔音机柜里面 专门的隔音机柜非常的贵 但是如果自己只买铁皮机柜加隔音棉那么会便宜一些 可能需要再加装一层外壳 透明的玻璃可能需要被更换成不透明的 同时改装之后需要留出专门的风道 风道内部塞管道隔音棉 风道出入口添加防尘网</p>
<p>to support multiple gpus, one must use pci-e extended cable. 128g per ram slot.</p>
<p>Dell r750 series, using dell riser card to connect gpu</p>
<p><a target="_blank" rel="noopener" href="https://github.com/hpcaitech/ColossalAI">https://github.com/hpcaitech/ColossalAI</a></p>
<p>for monsterious models, zero offload, pytorch loghtning, distributed training in pytorch, or deepspeed, fairscale, colossalai, Horovod is needed. no single gpu is able to hold gpt3-175B at once.</p>
<p>exporting to onnx:</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/serialization?highlight=onnx">https://huggingface.co/docs/transformers/serialization?highlight=onnx</a></p>
<p>lower model precision (quantization):</p>
<p>如果想要在GPU上操作，可以先使用torch.nn.export函数将模型转换成onnx格式，然后就可以放到TensorRT框架上inference了。（TensorRT目前不能直接解析Pytorch的网络模型，需要转换成onnx）</p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/cf83c877d71d">https://www.jianshu.com/p/cf83c877d71d</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zimiao552147572/article/details/105910915">https://blog.csdn.net/zimiao552147572/article/details/105910915</a></p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/quantization.html">https://pytorch.org/docs/stable/quantization.html</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/issues/14839">https://github.com/huggingface/transformers/issues/14839</a> (training gpt-j on colab)</p>
<h1 id="使用torch-quantization-quantize-dynamic获得动态量化的模型"><a href="#使用torch-quantization-quantize-dynamic获得动态量化的模型" class="headerlink" title="使用torch.quantization.quantize_dynamic获得动态量化的模型"></a>使用torch.quantization.quantize_dynamic获得动态量化的模型</h1><h1 id="量化的网络层为所有的nn-Linear的权重，使其成为int8"><a href="#量化的网络层为所有的nn-Linear的权重，使其成为int8" class="headerlink" title="量化的网络层为所有的nn.Linear的权重，使其成为int8"></a>量化的网络层为所有的nn.Linear的权重，使其成为int8</h1><p>quantized_model &#x3D; torch.quantization.quantize_dynamic(</p>
<p>model, {torch.nn.Linear}, dtype&#x3D;torch.qint8</p>
<p>)</p>
<h1 id="打印动态量化后的BERT模型"><a href="#打印动态量化后的BERT模型" class="headerlink" title="打印动态量化后的BERT模型"></a>打印动态量化后的BERT模型</h1><p>print(quantized_model)</p>
<p>how to use huggingface trainer:</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/363670628">https://zhuanlan.zhihu.com/p/363670628</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/486938936">https://zhuanlan.zhihu.com/p/486938936</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/358525654">https://zhuanlan.zhihu.com/p/358525654</a></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main_classes/deepspeed#custom-deepspeed-zero-inference">https://huggingface.co/docs/transformers/main_classes/deepspeed#custom-deepspeed-zero-inference</a></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main_classes/deepspeed">https://huggingface.co/docs/transformers/main_classes/deepspeed</a></p>
<p>zero offload requires sufficient RAM.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/alibaba/EasyParallelLibrary">https://github.com/alibaba/EasyParallelLibrary</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/SeanNaren/minGPT/tree/stage3">https://github.com/SeanNaren/minGPT/tree/stage3</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/prigoyal/pytorch_memonger/blob/master/tutorial/Checkpointing_for_PyTorch_models.ipynb">https://github.com/prigoyal/pytorch_memonger/blob/master/tutorial/Checkpointing_for_PyTorch_models.ipynb</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/eladrich/pixel2style2pixel">https://github.com/eladrich/pixel2style2pixel</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/EleutherAI/gpt-neox">https://github.com/EleutherAI/gpt-neox</a></p>
<p><a target="_blank" rel="noopener" href="https://www.eleuther.ai/">https://www.eleuther.ai</a></p>
<p>training turing-nlg:</p>
<p><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/">https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/</a></p>
<p>cited from deepspeed:</p>
<p>Extremely memory efficient: With just a single GPU, ZeRO-Offload of DeepSpeed can train models with over 10B parameters, 10x bigger than the state of the art, democratizing multi-billion-parameter model training such that many deep learning scientists can explore bigger and better models.</p>
<p>need p40&#x2F;m40 which has 24gb vram. need at least 60gb ram to load model.</p>
<p>using low ram devices need library like deepspeed, bminf or megengine.</p>
<p>you can also use others provided web services.</p>
<p>can use colab&#x2F;kaggle or aistudio to do the job. paid training enviorment is also avaliable.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/TsinghuaAI/CPM-1-Generate">https://github.com/TsinghuaAI/CPM-1-Generate</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/arrmansa/Basic-UI-for-GPT-J-6B-with-low-vram">https://github.com/arrmansa/Basic-UI-for-GPT-J-6B-with-low-vram</a></p>
<p><a target="_blank" rel="noopener" href="https://pythonawesome.com/finetune-gpt2-xl-and-gpt-neo-on-a-single-gpu-with-huggingface-transformers-using-deepspeed">https://pythonawesome.com/finetune-gpt2-xl-and-gpt-neo-on-a-single-gpu-with-huggingface-transformers-using-deepspeed</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/OpenBMB/BMInf">https://github.com/OpenBMB/BMInf</a></p>
<p>web api for chinese plug:</p>
<p><a target="_blank" rel="noopener" href="https://m.nlp.aliyun.com/mportal#/textGenerate">https://m.nlp.aliyun.com/mportal#/textGenerate</a></p>
<p>NVIDIA Tesla M40  24G 专业运算 英伟达 图形GPU加速深度学习显卡</p>
<p>提供魔改教程，需要一张亮机卡，用丽台K600当亮机卡就行，魔改后可用此卡打游戏</p>
<p>散热器可看图片，用3D打印散热风道，加上风扇就能用了</p>
<p>虚拟化 用这些卡 K1 K2 K340 K520（不需要授权） M60 P4 P10 P100 T4 RTX6000 RTX8000 V100 RTXA6000 P40 （需要授权）</p>
<p>虚拟化 VGPU 有两种模式 一种是VPC 适合普通办公 一种是VDWS 适合专业图形应用 然后P40两种模式都支持</p>
<p>购买之前先了解以下信息（您必须了解）：</p>
<p>1、电源供电有没有8针供电；</p>
<p>2、普通台式机X99以上主板，DDR4内存；</p>
<p>3、主板要支持多显卡显示；</p>
<p>4、建议电源功率在600W以上；</p>
<p>5、机箱内的空间是否足够（卡宽双挡片位置，卡长度28~30cm）</p>
<p>6、普通台式机需要加装主动散热，服务器可以选择被动散热</p>
<p>7、先查看主板bios的pcie设置里面有没有above 4g选项</p>
<p>超微服务器原装拆机 成色超新 测试好发货</p>
<p>普通台式机上Tesla M40显卡paddleGPU深度学习柯南的变身器上机体验</p>
<p>最近在paddlepaddle溜达，看到了柯南变身器，于是从aistudio下载到本地玩，单位的1060 6G版显卡，跑起来，语句一场就不行，遂上淘宝，咸鱼溜达一圈，见到了tesla k80 m40等一系列的卡卡。于是经过多番考虑（知乎一位买k80说翻车的帖子），于是最终下手m40。咸鱼有卖1800的，我问他保一个月质保不，他说，不保，我说谢谢，我考虑一下，他骂***（这时代也不知道怎么了，我就发了两条消息，这货好像脑子有点问题了，随后拉黑。因此不建议上咸鱼买，毕竟想上m40卡的应当希望稳妥一点）。随后在淘宝找了一家1945包邮的，还送一条转接线。挺合适，单买电源转接线需要30左右。（其实我怀疑这两家是一家，因为都是上海的）。淘宝这个质保3个月。当个保险。我用的机器配置如下，都是王牌(快玩完了的)产品。附上大师截图。大师我们需要它，他是给我们装显卡驱动的。win10自己好像不太认。以下配置，绝对可以跑paddlepaddleGPU框架。除了U，别的都挺便宜，刚开始买来做NAS的奈何功耗太高40w了，搁置了，现在加上m40满血复活。整套下来5000千元。当然，内存大家没必要这么大。4千完全够。我这主要是普通台式机使用m40，大家完全可以用二手服务器。买完之后，我发现网上很多说m40这些系列必须专门的主板和u才能跑，所以，那心情大家都能猜到，已经做好，再买板子的准备的我。</p>
<p>正文</p>
<p>两天到货了，同时购买的电源，没有收到，买的600W电源，没办法，偏远小城市，快递缓慢。</p>
<p>建议大家 买大于600w的电源，我原本一台没有独显的机器用的是300W电源，随机迫于心急之情，开始剪线，改电源。</p>
<p>改电源线</p>
<p>我原本的电源提供一个常规的显卡供电口，是6+2&#x3D;8的结构，3x12V 3xGND 2GND结构</p>
<p>而Tesla系列根据知乎朋友的介绍和，我的实际观察，确定其为一排4x12V 一排4xGND的结构，也就是和我的主板CPU供电一致。所以，知乎这位朋友说，他一开始用常规显卡接口供电，将他的k80干坏电容问题，估计很真实，也是这个前车之鉴，我小心对比了电源结构，最终开始剪掉了老板送电源线和我自己那个300W电源的12V显卡口，（其实一开始，准备只剪我自己这个电源的显卡口，改一下线路来着，奈何我这90块钱的电源，那线 细的就像头发丝 让我直接干断了）。接完线，我还发现我用的那个接口保护的热缩管，给小了，无奈，只能用电胶带缠绕。</p>

	
	</div>
  <a type="button" href="/blog/2022/04/28/48acae1c-c31d-4b17-ae00-97f8f1000a7f/#more" class="btn btn-default more">Read More</a>
</div>

	       
	     </div>
	     <div>
	       <center>
	         <div class="pagination">
<ul class="pagination">
	 
</ul>
</div>

	       </center>
	     </div>	
      

</div> <!-- col-md-9/col-md-12 -->


<div class="col-md-3">
	<div id="sidebar">
	
			
  <div id="site_search">
   <div class="form-group">
    <input type="text" id="local-search-input" name="q" results="0" placeholder="Search" class="st-search-input st-default-search-input form-control"/>
   </div>  
  <div id="local-search-result"></div>
  </div>


		
			
<div class="widget">
	<h4>Links</h4>
	<ul class="blogroll list-unstyled">
	
		<li><i class="fa fa-github"></i><a href="https://github.com/james4ever0/agi_computer_control/" title="Autonomous computer agent" target="_blank"]);">Project Cybergod</a></li>
	
		<li><i class="fa fa-github"></i><a href="https://github.com/james4ever0/pyjom/" title="Media content automation" target="_blank"]);">Project Pyjom</a></li>
	
		<li><i class="fa fa-github"></i><a href="https://github.com/james4ever0/prometheous/" title="Automated documentation, AI+IR(RAG)" target="_blank"]);">Project Prometheus</a></li>
	
		<li><i class="fa fa-github"></i><a href="https://github.com/james4ever0/pyjom/" title="Media Content Automation" target="_blank"]);">Project Pyjom</a></li>
	
		<li><i class="fa fa-github"></i><a href="https://github.com/james4ever0/my_blog_source/" title="Source code of my blog"" target="_blank"]);">Blog Source Code</a></li>
	
		<li><i class="fa fa-github"></i><a href="http://www.github.com/james4ever0" title="My Github account" target="_blank"]);">My Github</a></li>
	
		<li><i class="fa fa-book"></i><a href="https://samoyedsun.github.io/" title="Samoyedsun's Blog" target="_blank"]);">Samoyedsun&#39;s Blog</a></li>
	
		<li><i class="fa fa-book"></i><a href="http://atlant1c.cn/" title="Atlant1c's Blog" target="_blank"]);">Atlant1c&#39;s Blog</a></li>
	
		<li><i class="fa fa-book"></i><a href="https://www.gregoryuan.com/" title="Gregoryuan's Blog" target="_blank"]);">Gregoryuan&#39;s Blog</a></li>
	
		<li><i class="fa fa-book"></i><a href="https://yubingtao.netlify.app/" title="Yubingtao's Blog" target="_blank"]);">Yubingtao&#39;s Blog</a></li>
	
	</ul>
</div>


		
	</div> <!-- sidebar -->
</div> <!-- col-md-3 -->




    </div>
  </div>
  <div class="container-narrow">
    <footer> <p>
  &copy; 2024 James Brown
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p> </footer>
  </div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/blog/js/jquery.imagesloaded.min.js"></script>
<script src="/blog/js/gallery.js"></script>
<script src="/blog/js/bootstrap.min.js"></script>
<script src="/blog/js/main.js"></script>
<script src="/blog/js/search.js"></script> 


<link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/blog/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/blog/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>


<!-- syntax highlighting -->


</body>
</html>