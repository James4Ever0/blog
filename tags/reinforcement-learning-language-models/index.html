<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>reinforcement_learning_language_models | Blog of James Brown</title>
  <meta name="author" content="James Brown">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Blog of James Brown"/>

  
    <meta property="og:image" content=""/>
  

  
    <link rel="alternative" href="/blog/atom.xml" title="Blog of James Brown" type="application/atom+xml">
  
  
    <link href="/blog/favicon.png" rel="icon">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/blog/css/themes/cerulean.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/highlight-default.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/google-fonts.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/blog/css/comment.css" media="screen" type="text/css">
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.7/es5-sham.min.js"></script>
  <![endif]-->

  <script src="/blog/js/jquery-2.0.3.min.js"></script>
  
  
  <!-- analytics -->
  



<meta name="generator" content="Hexo 7.0.0"></head>

<body>
  <nav id="main-nav" class="navbar navbar-inverse navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
	<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="navbar-brand" href="/blog/">Blog of James Brown</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/blog/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/blog/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/blog/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/blog/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
    <div class="content">
      

<!-- title -->
<div class="page-header page-header-inverse ">
  <h1 class="archive-title-tag title title-inverse ">reinforcement_learning_language_models</h1>
</div>

<div class="row page">
  <!-- cols -->
  
  <div class="col-md-9">
	

	  <div id="top_search"></div>

      
         <!-- display as entry -->
	     <div class="mypage">
	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2022-12-06 </div>
			<div class="article-title"><a href="/blog/2022/12/06/ffec2fb0-c2fd-492e-8a84-8d8dd06c0096/" title="This article delves into the integration of generative question-answering and reinforcement learning in language models. It highlights projects such as OpenAI, CarperAI, Rallio67&#39;s dataset, KoboldAI, OPT, GPT-Neo, Sentence Transformers, ChatGPT deployment, and various marketing/customer service applications.">chatgpt</a></div>
		</h3>
	


		     
<div class="entry">

  <div class="row">
	
	
		<p>GPT4 is out.</p>
<hr>
<p><a target="_blank" rel="noopener" href="https://www.ooopn.com/tool/chatgpt/">三个国内镜像站</a>：</p>
<p><a target="_blank" rel="noopener" href="https://chat.forchange.cn/">https://chat.forchange.cn</a></p>
<p><a target="_blank" rel="noopener" href="https://aigcfun.com/">https://aigcfun.com</a></p>
<p><a target="_blank" rel="noopener" href="https://ai.askai.top/">https://ai.askai.top</a></p>
<hr>
<p>besides from decent processors, RAM and optimized runtime, in order to load LLMs fast, one would store the model weights on SSDs.</p>
<hr>
<p>now <a target="_blank" rel="noopener" href="https://github.com/hpcaitech/ColossalAI">colossalai</a> supports chatgpt training with a single gpu, using open-source code</p>
<p>check <a target="_blank" rel="noopener" href="https://app.humata.ai/">humata</a> for paper QA and information extraction&#x2F;language understanding from PDF files</p>
<hr>
<p>the syntax of chatgpt’s response is obviously markdown.</p>
<p>in order to be unblocked by chatgpt just because we are using static ip of corp’s wifi, we can connect through our phone’s hotspot.</p>
<p>Microsoft’s <a target="_blank" rel="noopener" href="https://bing.com/chat">EdgeGPT</a> needs you to open in Edge browser and join the waitlist of <a target="_blank" rel="noopener" href="https://bing.com/new">new Bing</a>, having 3rd party API <a target="_blank" rel="noopener" href="https://github.com/acheong08/EdgeGPT">here</a></p>
<p><a target="_blank" rel="noopener" href="http://getmerlin.in/">Merlin</a> is an extension based on ChatGPT which is avaliable for free and all countries, with 11 queries for free each day. Pro subscriptions incoming.</p>
<p>Rallio67 builds <a target="_blank" rel="noopener" href="https://github.com/Rallio67/language-model-agents">dataset for RLHF</a> and has released <a target="_blank" rel="noopener" href="https://huggingface.co/Rallio67">multiple chatgpt-like models</a> on huggingface. namely, <a target="_blank" rel="noopener" href="https://huggingface.co/Rallio67/joi_20B_instruct_alpha">joi</a>, <a target="_blank" rel="noopener" href="https://huggingface.co/Rallio67/chip_20B_instruct_alpha">chip</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/Rallio67/rosey_12B_instruct_alpha">rosey</a>, all based on <a target="_blank" rel="noopener" href="https://huggingface.co/EleutherAI/pythia-12b">pythia</a> or neox-20b. laion people tend to share loads to CPU in order to run these huge models properly.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/KoboldAI/KoboldAI-Client">KoboldAI</a> considered <a target="_blank" rel="noopener" href="https://huggingface.co/facebook/opt-2.7b">OPT</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/EleutherAI/gpt-neo-2.7B">GPT-Neo</a> as generic LMs. special models like NSFW shits may serve some purposes better.</p>
<p><a target="_blank" rel="noopener" href="https://alternativeto.net/software/chatgpt/">many alternatives</a>, but many are specialized in marketing and content generation, some are chatgpt replica, like <a target="_blank" rel="noopener" href="https://app.writesonic.com/">chatsonic</a> (with google knowledge) and <a target="_blank" rel="noopener" href="https://you.com/search?q=who+are+you&fromSearchBar=true&tbm=youchat">youchat</a> (from <a target="_blank" rel="noopener" href="https://you.com/">you.com</a> (awesome!))</p>
<p>open assistant now has a <a target="_blank" rel="noopener" href="https://open-assistant.io/dashboard">data collection website</a>, in which you can only perform tasks given and earn points (working for free? nah?)</p>
<p>it is adviced to run this chatgpt program with libraries instead of manually, to prevent issues.</p>
<p>my account has been banned from trying chatgpt. though it is not going to be free forever, you need to <a target="_blank" rel="noopener" href="https://github.com/golfzert/chatgpt-chinese-prompt-hack">moderate your input</a> (multi-language support, not only english but chinese) using some api to prevent similar incidents. also some topics outside of blacklist are banned intentionally so you need to check if the model is really producing the answer. if not you should avoid or change the way of asking it.</p>
<p>moderation via <a target="_blank" rel="noopener" href="https://platform.openai.com/docs/api-reference/moderations">official openai api</a>, <a target="_blank" rel="noopener" href="https://developers.perspectiveapi.com/s/?language=en_US">perspective api</a> (free), or via some projects like <a target="_blank" rel="noopener" href="https://github.com/fcakyon/content-moderation-deep-learning">content moderation deeplearning</a>, <a target="_blank" rel="noopener" href="https://github.com/NanoNets/bert-text-moderation">bert text moderation</a>, <a target="_blank" rel="noopener" href="https://huggingface.co/Hate-speech-CNERG/bert-base-uncased-hatexplain">bert-base-uncased-hatexplain</a>, <a target="_blank" rel="noopener" href="https://huggingface.co/unitary/toxic-bert">toxic-bert</a>, <a target="_blank" rel="noopener" href="https://github.com/surge-ai/copilot-toxicity/blob/main/copilot-toxicity.ipynb">copilot-toxicity</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/Andrazp/multilingual-hate-speech-robacofi">multilingual-hate-speech-robacofi</a>, train on datasets like <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/SetFit/hate_speech_offensive">hate_speech_offensive</a>, <a target="_blank" rel="noopener" href="https://github.com/surge-ai/toxicity">toxicity</a> (by surge-ai, a dataset labelling workforce) and <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/vidhur2k/multilingual-hate-speech/">multilingual-hate-speech</a></p>
<p>from my point of view, this is a service you cannot replicate at home, either requires smaller models with different architecture, or requires crowd-sourced computational power.</p>
<p>saying chatgpt is powered by <a target="_blank" rel="noopener" href="https://www.ray.io/">ray</a>, increasing parallelism.</p>
<p>bigscience <a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ?usp=sharing#scrollTo=VsXHWJLuowcn">petals colab</a> and <a target="_blank" rel="noopener" href="https://github.com/bigscience-workshop/petals">petals repo</a></p>
<p><a target="_blank" rel="noopener" href="https://discord.gg/xBPBXfcFHd">discord chatroom</a> for reproducing chatgpt</p>
<p>since many different models are derived from the original pretrained language model, <a target="_blank" rel="noopener" href="https://opendelta.readthedocs.io/en/latest/">opendelta</a> can save disk space by freezing main parameters, only tuning few of them.</p>
<p>this gpt seems really good. currently only api access.</p>
<p>but it is provided by openai which is no longer so “open” in the sense of “open-source”.</p>
<p><a target="_blank" rel="noopener" href="https://stability.ai/">stability.ai</a> is providing alternative open-source implementations of SOTA AI algorithms, which includes <a target="_blank" rel="noopener" href="https://carper.ai/">carper.ai</a>, <a target="_blank" rel="noopener" href="https://www.eleuther.ai/">eleuther.ai</a>, dreamstudio, <a target="_blank" rel="noopener" href="https://github.com/Harmonai-org/">harmonai</a> (audio), <a target="_blank" rel="noopener" href="https://laion.ai/">laion.ai</a> (datasets and projects)</p>
<h2 id="viable-approaches-to-chatgpt"><a href="#viable-approaches-to-chatgpt" class="headerlink" title="viable approaches to chatgpt"></a>viable approaches to chatgpt</h2><p>according to my point of view, chatgpt is just specialized on chat, or socialized in other words.</p>
<p>the elo rating system is the key to facebook social network, many zero-sum games. basically it is some revolution rating system. to do such rating system effectively one shall use along with classifiers and embeddings.</p>
<p>according to the training process of instructgpt and webgpt, we know that gpt has learned more by interacting with people (multiple QA), doing self-examination (learning a reward model) and performing actions (searching and quoting on web).</p>
<h3 id="RLHF"><a href="#RLHF" class="headerlink" title="RLHF"></a>RLHF</h3><h4 id="chainer-prompt-engineering"><a href="#chainer-prompt-engineering" class="headerlink" title="chainer, prompt engineering"></a>chainer, prompt engineering</h4><p><a target="_blank" rel="noopener" href="https://github.com/f/awesome-chatgpt-prompts">awesome chatgpt prompts</a></p>
<p><a target="_blank" rel="noopener" href="https://langchain.readthedocs.io/">langchain</a> extending llm by advanced prompts, llm wrappers actions, databases and memories</p>
<h4 id="RL-algorithms-tools-for-providing-feedback"><a href="#RL-algorithms-tools-for-providing-feedback" class="headerlink" title="RL algorithms, tools for providing feedback"></a>RL algorithms, tools for providing feedback</h4><p><a target="_blank" rel="noopener" href="https://github.com/andy-yangz/Awesome-RLHF">Awesome-RLHF</a> paper and code about RLHF</p>
<p><a target="_blank" rel="noopener" href="https://github.com/openai/baselines">openai baselines</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/DLR-RM/stable-baselines3">stable-baselines 3</a></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/blog/setfit">SetFit</a></p>
<p>Efficient few-shot learning with Sentence Transformers, used by <a target="_blank" rel="noopener" href="https://github.com/grantCelley/FewShotRLGPT">FewShotRLGPT</a> (no updates till now?)</p>
<h4 id="RLHF-models"><a href="#RLHF-models" class="headerlink" title="RLHF models"></a>RLHF models</h4><h5 id="non-language-models"><a href="#non-language-models" class="headerlink" title="non-language models"></a>non-language models</h5><p><a target="_blank" rel="noopener" href="https://github.com/anshradh/image_to_text_rlhf">image_to_text_rlhf</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/CarperAI/Algorithm-Distillation-RLHF">algorithm-distillation-rlhf</a></p>
<h5 id="language-models"><a href="#language-models" class="headerlink" title="language models"></a>language models</h5><p><a target="_blank" rel="noopener" href="https://github.com/BlinkDL/ChatRWKV">chatrwkv</a> pure rnn language model, with chinese support</p>
<p><a target="_blank" rel="noopener" href="https://github.com/conceptofmind/LaMDA-rlhf-pytorch">lamda-rlhf-chatgpt</a></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/facebook/blenderbot-3B">blenderbot2</a> a bot which can search internet, blenderbot3 is US only. install <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/ParlAI">ParlAI</a> then clone <a target="_blank" rel="noopener" href="https://github.com/JulesGM/ParlAI_SearchEngine">ParlAI_SearchEngine</a>. <a target="_blank" rel="noopener" href="https://dev.to/naruaika/how-i-managed-to-run-blenderbot-20-1jac">tutorial</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/clue-ai/PromptCLUE">promptCLUE</a> based on T5, created by <a target="_blank" rel="noopener" href="https://www.clueai.cn/doc">clueai</a>, trained on <a target="_blank" rel="noopener" href="https://github.com/CLUEbenchmark/pCLUE">pCLUE</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/LAION-AI/Open-Assistant">openassistant</a></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/mrsteyk/openchatgpt-neox-125m">openchatgpt-neox-125m</a> trained on chatgpt prompts, can be tested <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/mrsteyk/mrsteyk-openchatgpt-neox-125m">here</a>, trained from <a target="_blank" rel="noopener" href="https://huggingface.co/EleutherAI/pythia-6.7b-deduped">pythia</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/theblackcat102/copycat">copycat</a> chatgpt replicate</p>
<p><a target="_blank" rel="noopener" href="https://github.com/lucidrains/medical-chatgpt">medicine-chatgpt</a> shit sick of COVID-19</p>
<p><a target="_blank" rel="noopener" href="https://www.github.com/jordan-schneider/baby-rlhf/">baby-rlhf</a> both cartpole and languge model</p>
<p><a target="_blank" rel="noopener" href="https://github.com/ckkissane/rlhf-shakespeare">rlhf-shapespeare</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/voidful/TextRL">textrl</a> 100+stars</p>
<p><a target="_blank" rel="noopener" href="https://github.com/lucidrains/PaLM-rlhf-pytorch">PaLM-RLHF</a> claims <a target="_blank" rel="noopener" href="https://github.com/lucidrains/RETRO-pytorch">RETRO</a> will be integrated soon?</p>
<p><a target="_blank" rel="noopener" href="https://github.com/allenai/RL4LMs">RL4LMs</a> with multiple rl methods</p>
<p><a target="_blank" rel="noopener" href="https://github.com/thomfoster/minRLHF">minRLHF</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/mukulpatnaik/webgpt-cli/blob/main/webgpt.py">webgpt-cli</a> interface openai api to browse web and answer questions</p>
<p><a target="_blank" rel="noopener" href="https://github.com/openai/lm-human-preferences">lm-human-preferences</a> by openai</p>
<p><a target="_blank" rel="noopener" href="https://github.com/TheExGenesis/rlhf-magic">rlhf-magic</a> using <a target="_blank" rel="noopener" href="https://github.com/CarperAI/trlx">trlx</a> (supports GPT3-like models) which has PPO and <a target="_blank" rel="noopener" href="https://github.com/Sea-Snell/Implicit-Language-Q-Learning">ILQL</a> (as trainable model)</p>
<p><a target="_blank" rel="noopener" href="https://github.com/lvwerra/trl">trl</a> only has PPO on GPT2</p>
<p><a target="_blank" rel="noopener" href="https://github.com/yizhongw/Tk-Instruct">Tk-Instruct</a> T5 trained on natural instruct dataset. is it trained on RLHF systems?</p>
<h3 id="datasets"><a href="#datasets" class="headerlink" title="datasets"></a>datasets</h3><p><a target="_blank" rel="noopener" href="https://rootsignals.ai/W/all">whisperhub</a> collection of chatgpt prompts by plugin</p>
<p><a target="_blank" rel="noopener" href="https://github.com/anthropics/hh-rlhf">hh-rlhf</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/openai/following-instructions-human-feedback">instructgpt samples</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/allenai/natural-instructions">natural instructions</a></p>
<h3 id="dataset-building-tools"><a href="#dataset-building-tools" class="headerlink" title="dataset building tools"></a>dataset building tools</h3><p><a target="_blank" rel="noopener" href="https://github.com/SurfaceData/open-chatgpt-prompt-collective">open-chatgpt-prompt-collective</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Toloka/crowd-kit">crowd-kit</a> purify noisy data</p>
<p><a target="_blank" rel="noopener" href="https://github.com/bigscience-workshop/promptsource">promptsource</a></p>
<h3 id="reward-models"><a href="#reward-models" class="headerlink" title="reward models"></a>reward models</h3><p><a target="_blank" rel="noopener" href="https://github.com/martiansideofthemoon/rankgen">rankgen</a> scores model generations given a prefix (or prompt)</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/theblackcat102/electra-large-webgpt-rm">electra-webgpt-rm</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/theblackcat102/electra-large-reward-model">electra-large-reward-model</a> is based on <a target="_blank" rel="noopener" href="https://huggingface.co/google/electra-small-discriminator">electra</a> discriminator</p>
<h3 id="GPT3-like-models"><a href="#GPT3-like-models" class="headerlink" title="GPT3-like models"></a>GPT3-like models</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/facebook/galactica-1.3b">galactica</a> is opt trained on scientific data</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/bigscience/bloomz">bloomz</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/bigscience/mt0-large">mt0</a> trained on <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/bigscience/xP3">xP3</a> (multilingual prompts and code)</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/bigscience/T0pp">T0PP</a> T0 optimized for zero-shot prompts, despite much smaller than GPT-3</p>
<p><a target="_blank" rel="noopener" href="https://github.com/lucidrains/RETRO-pytorch">RETRO</a> another model with GPT-3 capabilities with fewer parameters?</p>
<p>gpt3 is gpt2 with <a target="_blank" rel="noopener" href="https://github.com/openai/sparse_attention">sparse attension</a>, which enables it to generate long sequence</p>
<p><a target="_blank" rel="noopener" href="https://github.com/XiangLi1999/Diffusion-LM">Diffusion-LM</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/lucidrains/PaLM-pytorch">PaLM</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/metaseq">metaseq</a> provides OPT, which is basically GPT3</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/togethercomputer/GPT-JT-6B-v1">GPT-JT</a> altered in many ways, trained on natural instructions <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/togethercomputer/GPT-JT">huggingface space</a></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/EleutherAI/gpt-neo-125M">GPT-Neo</a></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/model_doc/gptj">GPT-J</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/EleutherAI/gpt-neox">GPT-NeoX</a></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/bigscience/bloom">Bloom</a> large language model by bigscience</p>
<h3 id="autonomous-learning"><a href="#autonomous-learning" class="headerlink" title="autonomous learning"></a>autonomous learning</h3><p><a target="_blank" rel="noopener" href="https://autonomous-learning-library.readthedocs.io/en/stable/">autonomous-learning-library doc</a> and <a target="_blank" rel="noopener" href="https://github.com/cpnota/autonomous-learning-library">repo</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Gu-X">Gu-X</a> doing god-knows-what experiments</p>
<h2 id="analysis-about-how-to-make-such-model"><a href="#analysis-about-how-to-make-such-model" class="headerlink" title="analysis about how to make such model"></a>analysis about how to make such model</h2><p>gpt3 is capable of imitation (cause it is unsupervised.)</p>
<p>but! if you want to get things done (when you really need it!), you better want some aligned AI.</p>
<p>two similar models by openai: <a target="_blank" rel="noopener" href="https://openai.com/blog/webgpt/#samples">webgpt</a> and instructgpt</p>
<h3 id="about-instructgpt"><a href="#about-instructgpt" class="headerlink" title="about instructgpt"></a>about instructgpt</h3><p>it is first fine-tuned on supervised datasets, then train some reward model, then use the reward model to handle prompts and do reinforcement learning with PPO.</p>
<h3 id="details-on-webgpt-environment"><a href="#details-on-webgpt-environment" class="headerlink" title="details on webgpt environment"></a>details on webgpt environment</h3><p>guess: create states by performing actions, then generate templates to allow model filling blanks.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">Our text-based web-browsing environment is written mostly in Python with some JavaScript. For a</span><br><span class="line">high-level overview, see Section 2. Further details are as follows:</span><br><span class="line">• When a search is performed, we send the query to the Microsoft Bing Web Search API, and</span><br><span class="line">convert this to a simplified web page of results.</span><br><span class="line">• When a link to a new page is clicked, we call a Node.js script that fetches the HTML of the</span><br><span class="line">web page and simplifies it using Mozilla’s Readability.js.</span><br><span class="line">• We remove any search results or links to reddit.com or quora.com, to prevent the model</span><br><span class="line">copying answers from those sites.</span><br><span class="line">• We take the simplified HTML and convert links to the special format</span><br><span class="line">【&lt;link ID&gt;†&lt;link text&gt;†&lt;destination domain&gt;】, or</span><br><span class="line">【&lt;link ID&gt;†&lt;link text&gt;】 if the destination and source domains are the same. Here,</span><br><span class="line">the link ID is the index of the link on the page, which is also used for the link-clicking</span><br><span class="line">command. We use special characters such as 【 and 】 because they are rare and encoded</span><br><span class="line">in the same few ways by the tokenizer, and if they appear in the page text then we replace</span><br><span class="line">them by similar alternatives.</span><br><span class="line">• We convert superscripts and subscripts to text using ^ and _, and convert images to the</span><br><span class="line">special format [Image: &lt;alt text&gt;], or [Image] if there is no alt text.</span><br><span class="line">• We convert the remaining HTML to text using html2text.</span><br><span class="line">• For text-based content types other than HTML, we use the raw text. For PDFs, we convert</span><br><span class="line">them to text using pdfminer.six. For all other content types, and for errors and timeouts, we</span><br><span class="line">use an error message.</span><br><span class="line">• We censor any pages that contain a 10-gram overlap with the question (or reference answer,</span><br><span class="line">if provided) to prevent the model from cheating, and use an error message instead.</span><br><span class="line">• We convert the title of the page to text using the format &lt;page title&gt; (&lt;page domain&gt;).</span><br><span class="line">For search results pages, we use Search results for: &lt;query&gt;.</span><br><span class="line">• When a find in page or quote action is performed, we compare the text from the command</span><br><span class="line">against the page text with any links stripped (i.e., including only the text from each link).</span><br><span class="line">We also ignore case. For quoting, we also ignore whitespace, and allow the abbreviated</span><br><span class="line">format &lt;start text&gt;━&lt;end text&gt; to save tokens.</span><br><span class="line">• During browsing, the state of the browser is converted to text as shown in Figure 1(b).</span><br><span class="line">For the answering phase (the last step of the episode), we convert the question to</span><br><span class="line">text using the format &lt;question&gt;■, and follow this by each of the collected quotes</span><br><span class="line">in the format [&lt;quote number&gt;] &lt;quote page title&gt; (&lt;quote page domain&gt;)</span><br><span class="line">&lt;double new line&gt;&lt;quote extract&gt;■.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="projects-related-to-chatgpt"><a href="#projects-related-to-chatgpt" class="headerlink" title="projects related to chatgpt"></a>projects related to chatgpt</h2><h3 id="voice-assistants"><a href="#voice-assistants" class="headerlink" title="voice assistants"></a>voice assistants</h3><p><a target="_blank" rel="noopener" href="https://github.com/tiansztiansz/voice-assistant">voice assistant</a> in cpp</p>
<p><a target="_blank" rel="noopener" href="https://github.com/cjyaddone/ChatWaifu">ChatWaifu</a> with anime voice, <a target="_blank" rel="noopener" href="https://github.com/cjyaddone/ChatWaifuL2D">ChatWaifu with live2d</a></p>
<h3 id="hacking"><a href="#hacking" class="headerlink" title="hacking"></a>hacking</h3><p><a target="_blank" rel="noopener" href="https://github.com/daveshap/LongtermChatExternalSources">give longterm memory and external resources to gpt3</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/TheAppleTucker/backend-GPT">write backend logic with gpt</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/NoDataFound/hackGPT">hackgpt</a> exploit vulnerabilities</p>
<p><a target="_blank" rel="noopener" href="https://github.com/ke0z/VulChatGPT">vulchatgpt</a> ida plugin for reverse engineering</p>
<p><a target="_blank" rel="noopener" href="https://github.com/cedrickchee/chatgpt-universe">chatgpt-universe</a> things related to chatgpt</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Gv4y1z7tN/">galgame using chatgpt</a></p>
<p>记笔记</p>
<p>12.27更新了一个更精简的应用</p>
<p>强烈建议部署到服务器上</p>
<p>huggingface参考：<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Mahiruoshi/Lovelive-Nijigasaku-Chat-iSTFT-GPT3">https://huggingface.co/spaces/Mahiruoshi/Lovelive-Nijigasaku-Chat-iSTFT-GPT3</a></p>
<p>GitHub：<a target="_blank" rel="noopener" href="https://github.com/Paraworks/vits_with_chatgpt-gpt3">https://github.com/Paraworks/vits_with_chatgpt-gpt3</a></p>
<p>地址：<a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1vtootVMQ7wTOQwd15nJe6akzJUYNOw4d?usp=share_link">https://drive.google.com/drive/folders/1vtootVMQ7wTOQwd15nJe6akzJUYNOw4d?usp=share_link</a></p>
<p>你可以先尝试在服务器上部署，之后可以直接解压进文件夹后运行exe（mac、安卓端需要用renpy自行编译）</p>
<p>去<a target="_blank" rel="noopener" href="https://beta.openai.com/account/api-keys%E8%8E%B7%E5%8F%96api-key">https://beta.openai.com/account/api-keys获取api-key</a></p>
<p>参数照着敲就好了</p>
<p>人物id通常是从0开始的数字，我的模型最大到12</p>
<p>api部署方法：把inference_api.py放入你的vits目录下，进入文件修改config和checkpoint.pth的路径，比起应用程序来说十分简单，可以自行设计。码龄三个月写出的的雪山代码警告</p>
<p>——————————————————————————————————————————————————</p>
<p>Chatgpt部署方法已于12.26更新（视频后部分）</p>
<p>vits参考：<a target="_blank" rel="noopener" href="https://github.com/CjangCjengh/vits">https://github.com/CjangCjengh/vits</a></p>
<p>服务器端建议用ISTFT VITS：<a target="_blank" rel="noopener" href="https://github.com/innnky/MB-iSTFT-VITS">https://github.com/innnky/MB-iSTFT-VITS</a></p>
<p>model库：<a target="_blank" rel="noopener" href="https://github.com/CjangCjengh/TTSModels">https://github.com/CjangCjengh/TTSModels</a></p>
<p>也可以用我的<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Mahiruoshi/MIT-VITS-Nijigaku">https://huggingface.co/spaces/Mahiruoshi/MIT-VITS-Nijigaku</a></p>
<p>CHATGPT参考：<a target="_blank" rel="noopener" href="https://github.com/rawandahmad698/PyChatGPT">https://github.com/rawandahmad698/PyChatGPT</a></p>
<p>示例视频（纯服务器api，gpt3）<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1hP4y1B7wH/?spm_id_from=333.999.0.0&vd_source=7e8cf9f5c840ec4789ccb5657b2f0512">https://www.bilibili.com/video/BV1hP4y1B7wH/?spm_id_from=333.999.0.0&amp;vd_source=7e8cf9f5c840ec4789ccb5657b2f0512</a></p>
<p>穗乃果配音来自缪斯的模型﻿@Freeze_Phoenix</p>
<p>gpt3加载参考﻿@ぶらぶら散策中</p>
<p><a target="_blank" rel="noopener" href="https://github.com/jqueryscript/ChatGPT">chatgpt use cases curated list</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/mahaloz/DAILA">DAILA</a> use chatgpt</p>
<p>to identify function calls in decompiler</p>
<p><a target="_blank" rel="noopener" href="https://github.com/cedrickchee/awesome-transformer-nlp/blob/f2dbb46a382a30dd3d23d88c1b8d826ba5c34aab/README.md">awesome transformer language models</a> a huge collection on transformer based LMs, huge models by megacorps, with some introduction and analogy on chatgpt</p>
<p><a target="_blank" rel="noopener" href="https://github.com/huggingface/blog">huggingface blog</a> on <a target="_blank" rel="noopener" href="https://github.com/huggingface/blog/blob/dfbaec824b313f88581c761bc10c8943226970be/rlhf.md">RLHF</a> containing similar projects and source code</p>
<p>bilibili sends me lots of videos (and articles) on hacking and ai (including chatgpt) via its android app. recommend you to scrape this source and collect transcription and screenshots for searching and content generation.</p>
<p>b站有做免杀 绕过杀软的</p>
<p><a target="_blank" rel="noopener" href="https://bilibili.com/video/BV1B14y1K7AP">chatgpt原理解析</a></p>
<p>chatgpt对接搜索引擎</p>
<p>下载链接:</p>
<p>github: <a target="_blank" rel="noopener" href="https://github.com/josStorer/chat-gpt-search-engine-extension/releases/">https://github.com/josStorer/chat-gpt-search-engine-extension/releases/</a></p>
<p>百度网盘: <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1MnFJTDIatyIIPr5kUMWsAw?pwd=1111">https://pan.baidu.com/s/1MnFJTDIatyIIPr5kUMWsAw?pwd=1111</a></p>
<p>提取码：1111</p>
<p>原项目: <a target="_blank" rel="noopener" href="https://github.com/wong2/chat-gpt-google-extension">https://github.com/wong2/chat-gpt-google-extension</a></p>
<p>我创建的fork, 添加了多个搜索引擎支持的版本: <a target="_blank" rel="noopener" href="https://github.com/josStorer/chat-gpt-search-engine-extension">https://github.com/josStorer/chat-gpt-search-engine-extension</a></p>
<p>PR: <a target="_blank" rel="noopener" href="https://github.com/wong2/chat-gpt-google-extension/pull/31">https://github.com/wong2/chat-gpt-google-extension/pull/31</a></p>
<p>已修复先前百度需要手动刷新的问题</p>
<h2 id="access-via-api"><a href="#access-via-api" class="headerlink" title="access via api"></a>access via api</h2><p><a target="_blank" rel="noopener" href="https://github.com/altryne/chatGPT-telegram-bot">https://github.com/altryne/chatGPT-telegram-bot</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/taranjeet/chatgpt-api">https://github.com/taranjeet/chatgpt-api</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/acheong08/ChatGPT">https://github.com/acheong08/ChatGPT</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/vincelwt/chatgpt-mac">https://github.com/vincelwt/chatgpt-mac</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/transitive-bullshit/chatgpt-api">https://github.com/transitive-bullshit/chatgpt-api</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/rawandahmad698/PyChatGPT">https://github.com/rawandahmad698/PyChatGPT</a></p>
<h2 id="models-like-chatgpt"><a href="#models-like-chatgpt" class="headerlink" title="models like chatgpt"></a>models like chatgpt</h2><p><a target="_blank" rel="noopener" href="https://yjernite.github.io/lfqa.html">lfqa</a> retrival based generative QA</p>
<p><a target="_blank" rel="noopener" href="https://github.com/openai/lm-human-preferences">lm-human-preferences</a> by openai</p>
<p><a target="_blank" rel="noopener" href="https://github.com/lvwerra/trl">trl</a> Train transformer language models with reinforcement learning based on gpt2</p>
<p><a target="_blank" rel="noopener" href="https://github.com/CarperAI/trlx">trlx</a> A repo for distributed training of language models with Reinforcement Learning via Human Feedback (RLHF) by CarperAI</p>
<p><a target="_blank" rel="noopener" href="https://github.com/allenai/RL4LMs">RL4LMs</a> A modular RL library to fine-tune language models to human preferences</p>
<p><a target="_blank" rel="noopener" href="https://github.com/lucidrains/PaLM-rlhf-pytorch">PaLM-rlhf-pytorch</a> saying this is basically chatgpt with palm</p>
<p><a target="_blank" rel="noopener" href="https://github.com/lucidrains/g-mlp-gpt">gpt-gmlp</a> saying this design integrates gpt with gmlps so will use less ram and can be trained on a single gpu</p>
<p><a target="_blank" rel="noopener" href="https://github.com/James4Ever0/webgpt-cli">WebGPT</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/yizhongw/Tk-Instruct">tk-instruct</a> with <a target="_blank" rel="noopener" href="https://huggingface.co/models?search=allenai/tk-instruct">all models</a> by allenai can be <a target="_blank" rel="noopener" href="https://huggingface.co/allenai/mtk-instruct-11b-def-pos">multilingual</a>, trained on <a target="_blank" rel="noopener" href="https://instructions.apps.allenai.org/">natural instructions</a></p>
<p>there’s a ghosted repo named <a target="_blank" rel="noopener" href="https://github.com/mariusmcl/instructgpt-pytorch">instructgpt-pytorch</a> found in bing but no cache preserved, also an empty repo called <a target="_blank" rel="noopener" href="https://github.com/flippe3/InstructFNet">InstructFNet</a> wtf?</p>
<p><a target="_blank" rel="noopener" href="https://github.com/nicolas-lair/AidMe">AidMe</a> Code and experiment of the article AidMe User-in-the-loop Adaptative Intent Detecttion for Instructable Digital Assistant</p>
<p><a target="_blank" rel="noopener" href="https://github.com/CarperAI/cheese">cheese</a> Used for adaptive human in the loop evaluation of language and embedding models.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/AndRossi/Kelpie">Kelpie</a> Explainable AI framework for interpreting Link Predictions on Knowledge Graphs</p>
<p><a target="_blank" rel="noopener" href="https://github.com/archiki/GrIPS">GrIPS</a>  Gradient-free, Edit-based Instruction Search for Prompting Large Language Models</p>
<p><a target="_blank" rel="noopener" href="https://github.com/CarperAI/squeakily">queakily</a> nlp datasets cleaner</p>
<p>gpt-j</p>
<p>super big bilingual model <a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-130B">GLM-130B</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/JingfengYang/Multi-modal-Deep-Learning">multi-modal deeplearning</a> paper collections</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/model_doc/bloom">bloom</a> a huge model like gpt-3</p>
<p>notice, gpt-2 is somehow inferior to gpt-3 since it has smaller model parameters</p>
<p><a target="_blank" rel="noopener" href="https://github.com/bme-chatbots/dialogue-generation">dialogue-generation</a> Generating responses with pretrained XLNet and GPT-2 in PyTorch.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/illidanlab/personaGPT">personaGPT</a> Implementation of PersonaGPT Dialog Model</p>
<p><a target="_blank" rel="noopener" href="https://github.com/microsoft/DialoGPT">DialoGPT</a> Large-scale pretraining for dialogue</p>

	
	</div>
  <a type="button" href="/blog/2022/12/06/ffec2fb0-c2fd-492e-8a84-8d8dd06c0096/#more" class="btn btn-default more">Read More</a>
</div>

	       
	     </div>
	     <div>
	       <center>
	         <div class="pagination">
<ul class="pagination">
	 
</ul>
</div>

	       </center>
	     </div>	
      

</div> <!-- col-md-9/col-md-12 -->


<div class="col-md-3">
	<div id="sidebar">
	
			
  <div id="site_search">
   <div class="form-group">
    <input type="text" id="local-search-input" name="q" results="0" placeholder="Search" class="st-search-input st-default-search-input form-control"/>
   </div>  
  <div id="local-search-result"></div>
  </div>


		
			
<div class="widget">
	<h4>Links</h4>
	<ul class="blogroll list-unstyled">
	
		<li><i class="fa fa-github"></i><a href="https://github.com/james4ever0/agi_computer_control/" title="Autonomous computer agent" target="_blank"]);">Project Cybergod</a></li>
	
		<li><i class="fa fa-github"></i><a href="https://github.com/james4ever0/pyjom/" title="Media content automation" target="_blank"]);">Project Pyjom</a></li>
	
		<li><i class="fa fa-github"></i><a href="https://github.com/james4ever0/prometheous/" title="Automated documentation, AI+IR(RAG)" target="_blank"]);">Project Prometheus</a></li>
	
		<li><i class="fa fa-github"></i><a href="https://github.com/james4ever0/pyjom/" title="Media Content Automation" target="_blank"]);">Project Pyjom</a></li>
	
		<li><i class="fa fa-github"></i><a href="https://github.com/james4ever0/my_blog_source/" title="Source code of my blog"" target="_blank"]);">Blog Source Code</a></li>
	
		<li><i class="fa fa-github"></i><a href="http://www.github.com/james4ever0" title="My Github account" target="_blank"]);">My Github</a></li>
	
		<li><i class="fa fa-book"></i><a href="https://samoyedsun.github.io/" title="Samoyedsun's Blog" target="_blank"]);">Samoyedsun&#39;s Blog</a></li>
	
		<li><i class="fa fa-book"></i><a href="http://atlant1c.cn/" title="Atlant1c's Blog" target="_blank"]);">Atlant1c&#39;s Blog</a></li>
	
		<li><i class="fa fa-book"></i><a href="https://www.gregoryuan.com/" title="Gregoryuan's Blog" target="_blank"]);">Gregoryuan&#39;s Blog</a></li>
	
		<li><i class="fa fa-book"></i><a href="https://yubingtao.netlify.app/" title="Yubingtao's Blog" target="_blank"]);">Yubingtao&#39;s Blog</a></li>
	
	</ul>
</div>


		
	</div> <!-- sidebar -->
</div> <!-- col-md-3 -->




    </div>
  </div>
  <div class="container-narrow">
    <footer> <p>
  &copy; 2024 James Brown
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p> </footer>
  </div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/blog/js/jquery.imagesloaded.min.js"></script>
<script src="/blog/js/gallery.js"></script>
<script src="/blog/js/bootstrap.min.js"></script>
<script src="/blog/js/main.js"></script>
<script src="/blog/js/search.js"></script> 


<link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/blog/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/blog/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>


<!-- syntax highlighting -->


</body>
</html>